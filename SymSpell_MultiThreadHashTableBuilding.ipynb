{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from symspell import SymSpell\n",
    "import time\n",
    "import json\n",
    "from multiprocessing import Pool\n",
    "from itertools import islice\n",
    "import multiprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Traditional way of loading ==> Takes 7 mins\n",
    "ss = SymSpell(max_dictionary_edit_distance=3)\n",
    "\n",
    "#Multithread loading ==> with 4 threads, takes 3 mins\n",
    "ssMT = SymSpell(max_dictionary_edit_distance=3)\n",
    "\n",
    "#Number of threads equivalent to number of CPUs or cores\n",
    "NUM_PROCESSES=multiprocessing.cpu_count()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dictionary...\n",
      "Loaded dictionary...\n",
      "CPU times: user 7min 8s, sys: 1.95 s, total: 7min 10s\n",
      "Wall time: 7min 10s\n"
     ]
    }
   ],
   "source": [
    "#First load in traditional way. We are loading it as ss\n",
    "filename = 'SymSpell_Dctionary_Word_Full.json'\n",
    "#filename = 'test.json'\n",
    "\n",
    "%time ss.load_words_with_freq_from_json_and_build_dictionary(filename,encoding=\"ISO-8859-1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def buildDictMT(wordGroup):\n",
    "    \"\"\"\n",
    "    This function will be called from pool.\n",
    "    As it is multi thread, create its own SymSpell\n",
    "    At the end return _deletes\n",
    "    All pools will return its own _deletes\n",
    "    Threading function is expected to merge them together\n",
    "    \"\"\"\n",
    "    ssLocal = SymSpell(max_dictionary_edit_distance=3)\n",
    "    for word in wordGroup:\n",
    "        ssLocal.create_dictionary_entry_MT(word) #Dummy set word count. Later set correct count in master SymSpell\n",
    "    return ssLocal._deletes\n",
    "\n",
    "def chunks(data, SIZE=NUM_PROCESSES):\n",
    "    \"\"\"\n",
    "    Just create required chunks of raw words.\n",
    "    \"\"\"\n",
    "    it = iter(data)\n",
    "    for i in range(0, len(data), SIZE):\n",
    "        yield {k:data[k] for k in islice(it, SIZE)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadDataFromJSON(filename,ssL):\n",
    "    \"\"\"\n",
    "    Input is filename, expected to be already processed _words json.\n",
    "    We will first split the data into chunks\n",
    "    Create threads and process them in parllel\n",
    "    Result will be _deletes with hash key\n",
    "    As there can be similar hash keys across threads, we will merge them to our master SymSpell\n",
    "    Also we will update master SymSpell _words\n",
    "    \"\"\"\n",
    "    myData = dict()\n",
    "    \n",
    "    print(time.ctime()+': Loading words...')\n",
    "\n",
    "    with open(filename, 'r',encoding='ISO-8859-1') as fp:\n",
    "        myData = json.load(fp)\n",
    "    #To ensure single thread is not getting all worst cases, we will split them it to 4 times the core/threads\n",
    "    chunkSize=int(len(myData)/(4*NUM_PROCESSES))+1\n",
    "    print(time.ctime()+': Loaded %i words...' % len(myData))\n",
    "    # cut words into chunks, so that each chunk is processed in parallel\n",
    "    word_groups = chunks(myData,chunkSize )\n",
    "    #create pools\n",
    "    pool = Pool(NUM_PROCESSES)\n",
    "    # processes chunks in parallel\n",
    "    print(time.ctime()+': Building of dictionary started with %i threads...' % NUM_PROCESSES)\n",
    "    results = pool.map(buildDictMT, word_groups)\n",
    "    \n",
    "    #As map is blocking, we know all threads/processes are over\n",
    "    pool.close()\n",
    "    pool.terminate()\n",
    "    pool.join()  \n",
    "    print(time.ctime()+': Building of dictionary over...')\n",
    "    \n",
    "    #As each process would have given its own results, which might have duplicate hashes, merge them to our master\n",
    "    for result in results:\n",
    "        for hs in result:\n",
    "            if hs in ssL._deletes:\n",
    "                suggestions = ssL._deletes.get(hs)\n",
    "                suggestions.extend(result[hs])\n",
    "                ssL._deletes[hs] = suggestions\n",
    "            else:\n",
    "                ssL._deletes[hs] = result[hs]       \n",
    "    print(time.ctime()+': Copied result to master dictionary...')\n",
    "\n",
    "    #Push words and word counts to our master SymSpell\n",
    "    for word in myData:\n",
    "        ssL._words[word]=myData[word]\n",
    "        if len(word) > ssL._max_length:\n",
    "            ssL._max_length = len(word)\n",
    "    print(time.ctime()+': Copied words to master dictionary...')\n",
    "\n",
    "    del myData\n",
    "    del results\n",
    "    del pool\n",
    "    del word_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Apr 17 21:36:20 2018: Loading words...\n",
      "Tue Apr 17 21:36:21 2018: Loaded 500557 words...\n",
      "Tue Apr 17 21:36:21 2018: Building of dictionary started with 4 threads...\n",
      "Tue Apr 17 21:39:13 2018: Building of dictionary over...\n",
      "Tue Apr 17 21:39:30 2018: Copied result to master dictionary...\n",
      "Tue Apr 17 21:39:31 2018: Copied words to master dictionary...\n",
      "CPU times: user 1min 26s, sys: 2.98 s, total: 1min 29s\n",
      "Wall time: 3min 12s\n"
     ]
    }
   ],
   "source": [
    "#Multi thread loading\n",
    "#JSON is expected to be previously created unique _words json\n",
    "\n",
    "filename = 'SymSpell_Dctionary_Word_Full.json'\n",
    "#filename = 'test.json'\n",
    "%time loadDataFromJSON(filename,ssMT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "infinity:4601706:1\n",
      "heavily:8139465:2\n",
      "erroring:25:0\n",
      "CPU times: user 5.32 s, sys: 20 ms, total: 5.34 s\n",
      "Wall time: 5.34 s\n",
      "link is heavily erroring:25:-1\n"
     ]
    }
   ],
   "source": [
    "suggestion_list = ss.lookup(phrase='infifity', verbosity=1, max_edit_distance=2)\n",
    "for suggestion in suggestion_list:\n",
    "    print(suggestion)\n",
    "\n",
    "suggestion_list = ss.lookup(phrase='haevliy', verbosity=1, max_edit_distance=2)\n",
    "for suggestion in suggestion_list:\n",
    "    print(suggestion)\n",
    "\n",
    "suggestion_list = ss.lookup(phrase='erroring', verbosity=1, max_edit_distance=3)\n",
    "for suggestion in suggestion_list:\n",
    "    print(suggestion)\n",
    "\n",
    "%time suggestion_list = ss.lookup_compound(phrase='Link is haevliyy errorrinng', max_edit_distance=3)\n",
    "for suggestion in suggestion_list:\n",
    "    print(suggestion)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correctSentance(phrase, ssL):\n",
    "    suggestion_list = ssL.lookup_compound(phrase, max_edit_distance=3)\n",
    "    for suggestion in suggestion_list:\n",
    "        print(suggestion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "link is heavily erroring:25:-1\n",
      "CPU times: user 5.31 s, sys: 12 ms, total: 5.33 s\n",
      "Wall time: 5.33 s\n",
      "link is heavily erroring:25:-1\n",
      "CPU times: user 5.38 s, sys: 0 ns, total: 5.38 s\n",
      "Wall time: 5.38 s\n",
      "in to third quarter oblast jear he had learned orca secret plan a iran:1:-1\n",
      "CPU times: user 2.99 s, sys: 7 µs, total: 2.99 s\n",
      "Wall time: 2.99 s\n",
      "in to third quarter oblast jear he had learned orca secret plan a iran:1:-1\n",
      "CPU times: user 2.99 s, sys: 7.94 ms, total: 3 s\n",
      "Wall time: 3 s\n",
      "whereis to love head dated for much of the past who couldn't read in sixth grade and ins tired him:1:-1\n",
      "CPU times: user 4.09 s, sys: 8.03 ms, total: 4.1 s\n",
      "Wall time: 4.1 s\n",
      "whereis to love head dated for much of the past who couldn't read in sixth grade and ins tired him:1:-1\n",
      "CPU times: user 4.09 s, sys: 7.97 ms, total: 4.09 s\n",
      "Wall time: 4.09 s\n"
     ]
    }
   ],
   "source": [
    "phrase='Link is haevliyy errorrinng'\n",
    "%time correctSentance(phrase,ss)\n",
    "%time correctSentance(phrase,ssMT)\n",
    "phrase=\"in te dhird qarter oflast jear he hadlearned ofca sekretplan y iran\"\n",
    "%time correctSentance(phrase,ss)\n",
    "%time correctSentance(phrase,ssMT)\n",
    "phrase=\"whereis th elove hehad dated forImuch of thepast who couqdn'tread in sixthgrade and ins pired him\"\n",
    "%time correctSentance(phrase,ss)\n",
    "%time correctSentance(phrase,ssMT)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the biggest players in to str somme film slate ith plenty of fun:1:-1\n",
      "CPU times: user 6.45 s, sys: 16 µs, total: 6.45 s\n",
      "Wall time: 6.45 s\n"
     ]
    }
   ],
   "source": [
    "phrase=\"the bigjest playrs in te strogsommer film slatew ith plety of funn\"\n",
    "%time correctSentance(phrase,ssMT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2366064"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ss._deletes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2366064"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ssMT._deletes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ssMT._deletes == ss._deletes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving dictionary...\n",
      "Saved dictionary...\n",
      "CPU times: user 1min 38s, sys: 2 s, total: 1min 40s\n",
      "Wall time: 1min 40s\n"
     ]
    }
   ],
   "source": [
    "%time ss.save_complete_model_as_json(\"SymSpell_Dictionary_Word_500K_Complete_Model.json\",encoding=\"ISO-8859-1\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
